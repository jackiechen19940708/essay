故字节序问题是说对于超过8位的整数如何排布的问题，而对于像double和char数组这类类型，其实并没有影响。double一般是遵从IEEE标准的，对于各CPU都一致，而char数组更是由C代码计算地址的，不受字节序影响。所以首先要搞清楚，不是所有的东西都有字节序，而且字符序是以单字节为单位的顺序问题，不是字节内部的。

作者：孙笑凡
链接：https://www.zhihu.com/question/55140986/answer/142938758
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

MSB
LSB

Big-endian MSB在低地址，传输放在流的开始，mac java tcp
Little-endian MSB在高地址，传输放在留的末尾，x86

处理网络流（尤其涉及异构PC）需要特别注意

#include <iostream>
using namespace std;


int main(){

    int a = 0x12345678;
    long long aa = 0x0102030405060708;

    char c = *((char*)&a);
    char d = *(((char*)&aa));
    cout << (int)c << endl;
    cout << (int)d << endl;

}



Big endian's advantage:

Easier for (most) human to read when examining memory values. This sometimes also applies to serializing/deserializing values when communicating with networks.
Easier sign checking (by checking the byte at offset 0)
Easier comparison: useful in arbitrary-precision math, as numbers are compared from the most significant digit. But this is less important, because it’s not a common operation
No need for endianness conversion when sending/receiving data to/from the network. This is less useful because network adapters can already swap bytes and copy them to memory in the correct order without the help of the CPU, and most modern CPUs have the ability to swap bytes themselves
Little endian's advantage:

Easier parity checking (by checking the byte at offset 0 we can see that it’s odd or even)
Easier for some people to read: Arabic, Hebrew and many other languages write from right to left so they read numbers in little-endian order. Some languages also read number values in little-endian order (like 134 as 4 units, 3 tens and 1 hundred), so it’s easier to know how big the current digit is. That means the thousand separator is less useful to them, as we immediately know how big the current digit is
Natural in computation
Mathematics operations mostly work from least to most significant digit, so it's much easier to work in little-endian
This is extremely useful in Arbitrary-precision arithmetic (or any operations that are longer than the architecture's natural word size like doing 64-bit maths on 32-bit computers) because it would be much more painful to read the digits backwards and do operations
It’s also useful in situations like in case a computer with limited memory bandwidth (like some 32-bit ARM microcontrollers with 16-bit bus, or the Intel 8088 with 16-bit register but 8-bit data bus). Now the 32-bit CPU can do math 16 bits at a time by reading a halfword at address A, add it while still reading the remaining halfword at A+2 then do the final add instead of waiting for the two reads to be finished then adding from the LSB
Always reads as the same value if reading in the size less than or equal to the written value.
For example 20 = 0x14 if writing as a 64-bit value into memory at address A will be 14 00 00 00 00 00 00 00, and will always be read as 20 regardless of using 8, 16, 32, 64-bit reads (or actually any reads with length <= 64 at the address A like 24, 48 or 40 bits). This can be extended to arbitrarily longer types.
In big-endian system you have to know in which size you have written the value, in order to read it correctly. For example to get the least significant byte you need to read at byte A+n-1 (with n is the length in bytes of the write) instead of A.
This property also makes it easy to cast the value to a smaller type like int32_t to int16_t because the int16_t value will always lie at the beginning of int32_t.
In fact the designers of RISC-V architecture also said that:

We chose little-endian byte ordering for the RISC-V memory system because little-endian systems are currently dominant commercially (all x86 systems; iOS, Android, and Windows for ARM). A minor point is that we have also found little-endian memory systems to be more natural for hardware designers. However, certain application areas, such as IP networking, operate on big-endian data structures, and so we leave open the possibility of non-standard big-endian or bi-endian systems.

Some more discussions about that

Little-endian vs. big-endian
Big-Endian “is effectively dead”
https://softwareengineering.stac...
Why are both little- and big-endian in use?
